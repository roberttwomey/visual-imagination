{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwTP4MYk0bYn"
   },
   "source": [
    "# BigGAN + CLIP + CMA-ES\n",
    "\n",
    "[j.mp/bigclip](https://j.mp/bigclip)\n",
    "\n",
    "By Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com)\n",
    "\n",
    "Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\n",
    "\n",
    "Other CLIP notebooks: [OpenAI tutorial](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb), [SIREN by @advadnoun](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP), [SIREN by @norod78](https://colab.research.google.com/drive/1K1vfpTEvAmxW2rnhAaALRVyis8EiLOnD), [BigGAN by @advadnoun](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR), [BigGAN by @eyaler](j.mp/bigclip), [BigGAN by @tg_bomze](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v2.ipynb), [BigGAN using big-sleep library by @lucidrains](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb), [BigGAN story hallucinator by @bonkerfield](https://colab.research.google.com/drive/1jF8pyZ7uaNYbk9ZiVdxTOajkp8kbmkLK), [StyleGAN2-ADA Anime by @nagolinc](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb) [v2](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/CLIP_%2B_TADNE_(pytorch)_v2.ipynb)\n",
    "\n",
    "Using the works:\n",
    "\n",
    "https://github.com/openai/CLIP\n",
    "\n",
    "https://tfhub.dev/deepmind/biggan-deep-512\n",
    "\n",
    "https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
    "\n",
    "http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\n",
    "\n",
    "For a curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies: (UPDATE: all of this is moved to the gitlab image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "cellView": "form",
    "id": "EWmKTmvBg7z5"
   },
   "outputs": [],
   "source": [
    "# !nvidia-smi -L\n",
    "\n",
    "# import subprocess\n",
    "\n",
    "# CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "# print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "# if CUDA_version == \"10.0\":\n",
    "#     torch_version_suffix = \"+cu101\"\n",
    "# #     torch_version_suffix = \"+cu100\"\n",
    "# elif CUDA_version == \"10.1\":\n",
    "#     torch_version_suffix = \"+cu101\"\n",
    "# elif CUDA_version == \"10.2\":\n",
    "#     torch_version_suffix = \"\"\n",
    "# else:\n",
    "#     torch_version_suffix = \"+cu110\"\n",
    "\n",
    "# !pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "cellView": "form",
    "id": "1SDpkkK7cU1y",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install pytorch-pretrained-biggan\n",
    "# !pip install nltk\n",
    "# !pip install cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install code repo for CLIP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'CLIP'...\n",
      "remote: Enumerating objects: 21, done.\u001b[K\n",
      "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
      "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
      "remote: Total 21 (delta 0), reused 14 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (21/21), done.\n"
     ]
    }
   ],
   "source": [
    "# !git clone --depth 1 https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 235773527/235773527 [00:05<00:00, 42121615.51B/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 497029.06B/s]\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pretrained_biggan import BigGAN\n",
    "last_gen_model = 'biggan-deep-512'\n",
    "biggan_model = BigGAN.from_pretrained(last_gen_model).cuda().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/CLIP\n"
     ]
    }
   ],
   "source": [
    "cd CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████| 353976522/353976522 [00:19<00:00, 17853585.76it/s]\n"
     ]
    }
   ],
   "source": [
    "import clip\n",
    "last_clip_model = 'ViT-B/32'\n",
    "perceptor, preprocess = clip.load(last_clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outpath = \"/home/jovyan/work/output/\"\n",
    "!rm -rf $outpath\n",
    "!mkdir -p $outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt = 'television buddha sculpture with grass'\n",
    "# prompt = 'a buddha sculpture with television in the grass'\n",
    "# prompt = 'buddha television grass'\n",
    "# prompt = \"a clean well-lighted place\"\n",
    "# prompt = \"large bright white room fluorescent light\"\n",
    "prompt = \"a large, bright, white room\"\n",
    "gen_model = 'biggan-deep' #@param ['biggan-deep', 'sigmoid']\n",
    "size = '512' #@param [512, 256, 128] \n",
    "color = True #@param {type:'boolean'}\n",
    "initial_class = 'Random mix' #@param ['From prompt', 'Random class', 'Random Dirichlet', 'Random mix', 'Random embeddings'] {allow-input: true}\n",
    "optimize_class = True #@param {type:'boolean'}\n",
    "class_smoothing = 0.1 #@param {type:'number'}\n",
    "truncation = 1 #@param {type:'number'}\n",
    "stochastic_truncation = False #@param {type:'boolean'}\n",
    "optimizer = 'CMA-ES' #@param ['SGD','Adam','CMA-ES','CMA-ES+SGD','CMA-ES+Adam']\n",
    "pop_size = 50 #@param {type:'integer'}\n",
    "clip_model = 'ViT-B/32' #@param ['ViT-B/32','RN50']\n",
    "augmentations =  64#@param {type:'integer'}\n",
    "learning_rate =  0.1#@param {type:'number'}\n",
    "standartization_loss =  0#@param {type:'number'}\n",
    "minimum_entropy_loss = 0.0001 #@param {type:'number'}\n",
    "embeddings_l2_loss = 0.0001 #@param {type:'number'}\n",
    "total_variation_loss = 0.1 #@param {type:'number'}\n",
    "iterations = 100 #@param {type:'integer'}\n",
    "save_every = 1 #@param {type:'integer'}\n",
    "fps = 1 #@param {type:'number'}\n",
    "freeze_secs = 0 #@param {type:'number'}\n",
    "\n",
    "# seed =  0#@param {type:'number'}\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "form",
    "id": "AOWzPLrBbdxW"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample=99 iter=99 best: total=67.31 cos=67.31 reg=0.000 avg: total=68.76 cos=68.75 reg=0.009 std: total=0.89 cos=0.89 reg=0.002 1st=Indian_elephant(0.47) 2nd=microwave(0.41) 3rd=gondola(0.10) components: >=0.5:1, >=0.3:1, >=0.1:1\n",
      "took: 521 secs (5.22 sec/iter) on GPU 0: GeForce RTX 2080 Ti (UUID: GPU-ddd28d43-f6d6-b109-02c9-a300fd6d3c47)\n"
     ]
    }
   ],
   "source": [
    "#@title Generate!\n",
    "#@markdown 1. For **prompt** OpenAI suggest to use the template \"A photo of a X.\" or \"A photo of a X, a type of Y.\" [[paper]](https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf)\n",
    "#@markdown 2. For **initial_class** you can either use free text or select a special option from the drop-down list.\n",
    "#@markdown 3. Free text and 'From prompt' might fail to find an appropriate ImageNet class.\n",
    "#@markdown 4. **seed**=0 means no seed.\n",
    "\n",
    "if seed == 0:\n",
    "  seed = None\n",
    "noise_size = 128\n",
    "class_size = 128 if initial_class.lower()=='random embeddings' else 1000\n",
    "channels = 3 if color else 1\n",
    "clip_res = perceptor.input_resolution.item()\n",
    "gen_model = gen_model + '-' + size\n",
    "sideX = sideY = int(size)\n",
    "if sideX<=clip_res and sideY<=clip_res:\n",
    "  augmentations = 1\n",
    "if 'CMA' not in optimizer:\n",
    "  pop_size = 1\n",
    "if gen_model != last_gen_model and 'biggan' in gen_model:\n",
    "  biggan_model = BigGAN.from_pretrained(gen_model).cuda().eval()\n",
    "  last_gen_model = gen_model\n",
    "if clip_model != last_clip_model:\n",
    "  perceptor, preprocess = clip.load(clip_model)\n",
    "  last_clip_model = clip_model\n",
    "if 'sigmoid' in gen_model:\n",
    "  optimize_class = False\n",
    "emb_factor = 0.005\n",
    "\n",
    "def my_forward(self, z, class_label, truncation):\n",
    "  assert 0 < truncation <= 1\n",
    "\n",
    "  if initial_class.lower()=='random embeddings':\n",
    "    embed = class_label\n",
    "  else:\n",
    "    embed = self.embeddings(class_label)\n",
    "    \n",
    "  cond_vector = torch.cat((z, embed), dim=1)\n",
    "\n",
    "  z = self.generator(cond_vector, truncation)\n",
    "  return z\n",
    "\n",
    "BigGAN.forward = my_forward\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "state = None if not seed else np.random.RandomState(seed)\n",
    "np.random.seed(seed)\n",
    "import torch\n",
    "import torchvision\n",
    "import sys\n",
    "torch.manual_seed(np.random.randint(sys.maxsize))\n",
    "import imageio\n",
    "from IPython.display import HTML, Image, clear_output\n",
    "from scipy.stats import truncnorm, dirichlet\n",
    "from pytorch_pretrained_biggan import BigGAN, convert_to_images, one_hot_from_names, utils\n",
    "from nltk.corpus import wordnet as wn\n",
    "from base64 import b64encode\n",
    "from time import time\n",
    "import cma\n",
    "from cma.sigma_adaptation import CMAAdaptSigmaCSA, CMAAdaptSigmaTPA\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", cma.evolution_strategy.InjectionWarning)\n",
    "\n",
    "ind2name = {index: wn.of2ss('%08dn'%offset).lemma_names()[0] for offset, index in utils.IMAGENET.items()}\n",
    "\n",
    "def save(out,name):\n",
    "  with torch.no_grad():\n",
    "    out = out.cpu().numpy()\n",
    "  if 'sigmoid' in gen_model:\n",
    "    out = out*2 - 1\n",
    "  img = convert_to_images(out)[0]\n",
    "  imageio.imwrite(name, np.asarray(img))\n",
    "\n",
    "def checkin(i, best_ind, total_losses, losses, regs, out, probs=None):\n",
    "  global sample_num\n",
    "  name = outpath+'frame_%05d.jpg'%sample_num\n",
    "  save(out, name)\n",
    "  clear_output()\n",
    "  display(Image(name))  \n",
    "  stats = 'sample=%d iter=%d best: total=%.2f cos=%.2f reg=%.3f avg: total=%.2f cos=%.2f reg=%.3f std: total=%.2f cos=%.2f reg=%.3f'%(sample_num, i, total_losses[best_ind], losses[best_ind], regs[best_ind], np.mean(total_losses), np.mean(losses), np.mean(regs), np.std(total_losses), np.std(losses), np.std(regs))\n",
    "  if probs is not None:\n",
    "    best = probs[best_ind]\n",
    "    inds = np.argsort(best)[::-1]\n",
    "    probs = np.array(probs)\n",
    "    stats += ' 1st=%s(%.2f) 2nd=%s(%.2f) 3rd=%s(%.2f) components: >=0.5:%.0f, >=0.3:%.0f, >=0.1:%.0f'%(ind2name[inds[0]], best[inds[0]], ind2name[inds[1]], best[inds[1]], ind2name[inds[2]], best[inds[2]], np.sum(probs >= 0.5)/pop_size,np.sum(probs >= 0.3)/pop_size,np.sum(probs >= 0.1)/pop_size)\n",
    "  print(stats)\n",
    "  sample_num += 1\n",
    "\n",
    "eps = 1e-8\n",
    "if 'sigmoid' in gen_model:\n",
    "  noise_size = channels*sideY*sideX\n",
    "  noise_vector = np.random.rand(pop_size, noise_size).astype(np.float32)\n",
    "  noise_vector = np.log((noise_vector+eps)/(1-noise_vector+eps))\n",
    "else:\n",
    "  noise_vector = truncnorm.rvs(-2*truncation, 2*truncation, size=(pop_size, noise_size), random_state=state).astype(np.float32) #see https://github.com/tensorflow/hub/issues/214\n",
    "\n",
    "  if initial_class.lower() == 'random class':\n",
    "    class_vector = np.ones(shape=(pop_size, class_size), dtype=np.float32)*class_smoothing/999\n",
    "    class_vector[0,np.random.randint(class_size)] = 1-class_smoothing\n",
    "  elif initial_class.lower() == 'random dirichlet':\n",
    "    class_vector = dirichlet.rvs([pop_size/class_size] * class_size, size=1, random_state=state).astype(np.float32)\n",
    "  elif initial_class.lower() == 'random mix':\n",
    "    class_vector = np.random.rand(pop_size, class_size).astype(np.float32)\n",
    "  elif initial_class.lower() == 'random embeddings':\n",
    "    class_vector = np.random.randn(pop_size, class_size).astype(np.float32)\n",
    "  else:\n",
    "    if initial_class.lower() == 'from prompt':\n",
    "      initial_class = prompt\n",
    "    try:\n",
    "      class_vector = None\n",
    "      class_vector = one_hot_from_names(initial_class, batch_size=pop_size)\n",
    "      assert class_vector is not None\n",
    "      class_vector = class_vector*(1-class_smoothing*class_size/(class_size-1))+class_smoothing/(class_size-1)\n",
    "    except Exception as e:  \n",
    "      print('Error: could not find initial_class. Try something else.')\n",
    "      raise e\n",
    "\n",
    "  if initial_class.lower() != 'random embeddings':\n",
    "    class_vector = class_vector/np.sum(class_vector,axis=-1, keepdims=True)\n",
    "    class_vector = np.log(class_vector+eps)\n",
    "  initial_class_vector = class_vector[0]\n",
    "  if initial_class.lower() == 'random mix':\n",
    "    initial_class_vector = initial_class_vector*0-np.log(class_size)\n",
    "  if initial_class.lower() == 'random embeddings':\n",
    "    initial_class_vector = initial_class_vector*0\n",
    "  class_vector = torch.tensor(class_vector, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\n",
    "  smoothed_ent = -torch.tensor(class_smoothing*np.log(class_smoothing/999+eps)+(1-class_smoothing)*np.log(1-class_smoothing+eps), dtype=torch.float32).cuda()\n",
    "noise_vector = torch.tensor(noise_vector, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\n",
    "\n",
    "if 'SGD' in optimizer or 'Adam' in optimizer:\n",
    "  params = [noise_vector]\n",
    "  if optimize_class:\n",
    "    params = params + [class_vector]\n",
    "  if 'SGD' in optimizer:\n",
    "    optim = torch.optim.SGD(params, lr=learning_rate, momentum=0.9)  \n",
    "  else:\n",
    "    optim = torch.optim.Adam(params, lr=learning_rate)\n",
    "\n",
    "tx = clip.tokenize(prompt)\n",
    "with torch.no_grad():\n",
    "  target_clip = perceptor.encode_text(tx.cuda())\n",
    "\n",
    "def get_output(noise_vector, class_vector):\n",
    "  if stochastic_truncation:\n",
    "    with torch.no_grad():\n",
    "      trunc_indices = noise_vector.abs() > 2*truncation\n",
    "      size = torch.count_nonzero(trunc_indices).cpu().numpy()\n",
    "      trunc = truncnorm.rvs(-2*truncation, 2*truncation, size=(1,size)).astype(np.float32)\n",
    "      noise_vector.data[trunc_indices] = torch.tensor(trunc, requires_grad='SGD' in optimizer or 'Adam' in optimizer, device='cuda')\n",
    "  else:\n",
    "    noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "  if initial_class.lower() == 'random embeddings':\n",
    "    class_vector_norm = class_vector*emb_factor\n",
    "  else:\n",
    "    class_vector_norm = class_vector.softmax(dim=-1)\n",
    "  return biggan_model(noise_vector, class_vector_norm, truncation), class_vector_norm\n",
    "\n",
    "def ascend_txt(i, grad_step=False, show_save=False):\n",
    "  prev_class_vector_norms = []\n",
    "  regs = []\n",
    "  losses = []\n",
    "  total_losses = []\n",
    "  best_loss = np.inf\n",
    "  for j in range(pop_size):\n",
    "    p_s = []\n",
    "    if 'sigmoid' in gen_model:\n",
    "      out = noise_vector[j:j+1].sigmoid().reshape(1, channels, sideY, sideX)\n",
    "      prev_class_vector_norms = None\n",
    "    else:\n",
    "      out, class_vector_norm = get_output(noise_vector[j:j+1], class_vector[j:j+1])\n",
    "      if channels==1:\n",
    "        out = out.mean(dim=1, keepdim=True)\n",
    "      if initial_class.lower() == 'random embeddings':\n",
    "        prev_class_vector_norms = None\n",
    "      else:\n",
    "        with torch.no_grad():\n",
    "          prev_class_vector_norms.append(class_vector_norm.cpu().numpy()[0])\n",
    "    if channels==1:\n",
    "      out = out.repeat(1,3,1,1)\n",
    "    \n",
    "    for aug in range(augmentations):\n",
    "      if sideX<=clip_res and sideY<=clip_res:\n",
    "        apper = out  \n",
    "      else:\n",
    "        size = torch.randint(int(.5*sideX), int(.98*sideX), ())\n",
    "        #size = int(sideX*torch.zeros(1,).normal_(mean=.8, std=.3).clip(.5, .95))\n",
    "        offsetx = torch.randint(0, sideX - size, ())\n",
    "        offsety = torch.randint(0, sideX - size, ())\n",
    "        apper = out[:, :, offsetx:offsetx + size, offsety:offsety + size]\n",
    "        apper = (apper+1)/2\n",
    "      apper = torch.nn.functional.interpolate(apper, clip_res, mode='bicubic')\n",
    "      apper = apper.clamp(0,1)\n",
    "      p_s.append(apper)\n",
    "    into = nom(torch.cat(p_s, 0))\n",
    "    predict_clip = perceptor.encode_image(into)\n",
    "    factor = 100\n",
    "    loss = factor*(1-torch.cosine_similarity(predict_clip, target_clip).mean())\n",
    "    total_loss = loss\n",
    "    if 'sigmoid' in gen_model and total_variation_loss or 'biggan' in gen_model and (standartization_loss or optimize_class and (initial_class.lower() != 'random embeddings' and minimum_entropy_loss or  initial_class.lower() == 'random embeddings' and embeddings_l2_loss)):\n",
    "      reg = 0\n",
    "      if 'sigmoid' in gen_model:\n",
    "        if total_variation_loss:\n",
    "          reg += total_variation_loss*((out[:, :, :-1, :] - out[:, :, 1:, :]).abs().mean() + (out[:, :, :, :-1] - out[:, :, :, 1:]).abs().mean())\n",
    "      elif 'biggan' in gen_model:\n",
    "        if minimum_entropy_loss and initial_class.lower() != 'random embeddings':\n",
    "          reg += minimum_entropy_loss*((-class_vector_norm*torch.log(class_vector_norm+eps)).sum()-smoothed_ent).abs()\n",
    "        elif embeddings_l2_loss and initial_class.lower() == 'random embeddings':\n",
    "          reg += embeddings_l2_loss*class_vector_norm.square().sum()\n",
    "        if standartization_loss: #https://arxiv.org/abs/1903.00925\n",
    "          mu2 = noise_vector[j:j+1].mean().square()\n",
    "          sigma2 = noise_vector[j:j+1].std().square()\n",
    "          reg += standartization_loss*(mu2+sigma2-torch.log(sigma2))\n",
    "      reg = factor*reg\n",
    "      total_loss = total_loss + reg\n",
    "      with torch.no_grad():\n",
    "        regs.append(reg.item())\n",
    "    else:\n",
    "      regs.append(0)\n",
    "    with torch.no_grad():\n",
    "      losses.append(loss.item())\n",
    "      total_losses.append(total_loss.item())\n",
    "    if total_losses[-1]<best_loss:\n",
    "      best_loss = total_losses[-1]\n",
    "      best_ind = j\n",
    "      best_out = out\n",
    "    if grad_step:    \n",
    "      optim.zero_grad()\n",
    "      total_loss.backward()\n",
    "      optim.step()\n",
    "      \n",
    "  if show_save and (i == iterations-1 or i % save_every == 0):\n",
    "    if i==iterations-1:\n",
    "      save(best_out,'%s.jpg'%prompt)  \n",
    "    if i % save_every == 0:\n",
    "      checkin(i, best_ind, total_losses, losses, regs, best_out, prev_class_vector_norms)  \n",
    "  return total_losses\n",
    "\n",
    "nom = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "if 'CMA' in optimizer:\n",
    "  initial_vector = np.zeros(noise_size)\n",
    "  bounds = None\n",
    "  #if 'sigmoid' not in gen_model and not stochastic_truncation:\n",
    "  #  bounds = [-2*truncation*np.ones(noise_size),2*truncation*np.ones(noise_size)]\n",
    "  if optimize_class:\n",
    "    initial_vector = np.hstack([initial_vector, initial_class_vector])\n",
    "    #if not stochastic_truncation:\n",
    "    #  bounds[0] = list(bounds[0]) + [None]*class_size\n",
    "    #  bounds[1] = list(bounds[1]) + [None]*class_size\n",
    "  cma_opts = {'popsize': pop_size, 'seed': np.nan, 'AdaptSigma': True, 'CMA_diagonal': True, 'CMA_active': False, 'CMA_elitist':False, 'bounds':bounds}\n",
    "  cmaes = cma.CMAEvolutionStrategy(initial_vector, 1, inopts=cma_opts)\n",
    "\n",
    "sample_num = 0\n",
    "machine = !nvidia-smi -L\n",
    "start = time()\n",
    "for i in range(iterations):    \n",
    "  if 'CMA' in optimizer:\n",
    "    with torch.no_grad():\n",
    "      cma_results = torch.tensor(cmaes.ask(), dtype=torch.float32).cuda()\n",
    "      if optimize_class:\n",
    "        noise_vector.data, class_vector.data = torch.split_with_sizes(cma_results, (noise_size, class_size), dim=-1)\n",
    "        class_vector.data = class_vector.data\n",
    "      else:\n",
    "        noise_vector.data = cma_results      \n",
    "  if 'SGD' in optimizer or 'Adam' in optimizer:\n",
    "    losses = ascend_txt(i, grad_step=True, show_save='CMA' not in optimizer)\n",
    "    assert noise_vector.requires_grad and noise_vector.is_leaf and (not optimize_class or class_vector.requires_grad and class_vector.is_leaf), (noise_vector.requires_grad, noise_vector.is_leaf, class_vector.requires_grad, class_vector.is_leaf)\n",
    "  if 'CMA' in optimizer:\n",
    "    with torch.no_grad():\n",
    "      losses = ascend_txt(i, show_save=True)\n",
    "      if optimize_class:\n",
    "        vectors = torch.cat([noise_vector,class_vector], dim=1)\n",
    "      else:\n",
    "        vectors = noise_vector\n",
    "      cmaes.tell(vectors.cpu().numpy(), losses)\n",
    "  if i == iterations-1 or i % save_every == 0:\n",
    "    print('took: %d secs (%.2f sec/iter) on %s'%(time()-start,(time()-start)/(i+1), machine[0]))\n",
    "\n",
    "# from google.colab import files, output\n",
    "# files.download('output/%s.jpg'%prompt)\n",
    "\n",
    "# out = '\"output/%s.mp4\"'%prompt\n",
    "# with open('output/list.txt','w') as f:\n",
    "#   for i in range(sample_num):\n",
    "#     f.write('file output/frame_%05d.jpg\\n'%i)\n",
    "#   for j in range(int(freeze_secs*fps)):\n",
    "#     f.write('file output/frame_%05d.jpg\\n'%i)\n",
    "# !ffmpeg -r $fps -f concat -safe 0 -i /content/list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
    "# with open('/%s.mp4'%prompt, 'rb') as f:\n",
    "#   data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
    "# display(HTML(\"\"\"\n",
    "#   <video controls autoplay loop>\n",
    "#         <source src=\"%s\" type=\"video/mp4\">\n",
    "#   </video>\"\"\" % data_url))\n",
    "\n",
    "# from google.colab import files, output\n",
    "# output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
    "# files.download('output/%s.mp4'%prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save(out,'%s.jpg'%prompt.replace(\" \", \"_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/output\n"
     ]
    }
   ],
   "source": [
    "cd $outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/work/output'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg version 3.4.8-0ubuntu0.2 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 7 (Ubuntu 7.5.0-3ubuntu1~18.04)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.2 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --enable-gpl --disable-stripping --enable-avresample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librubberband --enable-librsvg --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-omx --enable-openal --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libopencv --enable-libx264 --enable-shared\n",
      "  libavutil      55. 78.100 / 55. 78.100\n",
      "  libavcodec     57.107.100 / 57.107.100\n",
      "  libavformat    57. 83.100 / 57. 83.100\n",
      "  libavdevice    57. 10.100 / 57. 10.100\n",
      "  libavfilter     6.107.100 /  6.107.100\n",
      "  libavresample   3.  7.  0 /  3.  7.  0\n",
      "  libswscale      4.  8.100 /  4.  8.100\n",
      "  libswresample   2.  9.100 /  2.  9.100\n",
      "  libpostproc    54.  7.100 / 54.  7.100\n",
      "Input #0, concat, from 'list.txt':\n",
      "  Duration: N/A, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: mjpeg, yuvj420p(pc, bt470bg/unknown/unknown), 512x512 [SAR 1:1 DAR 1:1], 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "\u001b[1;34m[swscaler @ 0x55ddb6518920] \u001b[0m\u001b[0;33mdeprecated pixel format used, make sure you did set range correctly\n",
      "\u001b[0m\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0musing SAR=1/1\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0musing cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mprofile Constrained Baseline, level 2.2\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0m264 - core 152 r2854 e9a5903 - H.264/MPEG-4 AVC codec - Copyleft 2003-2017 - http://www.videolan.org/x264.html - options: cabac=0 ref=3 deblock=1:0:0 analyse=0x1:0x111 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=0 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=16 lookahead_threads=2 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=0 weightp=0 keyint=250 keyint_min=1 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'a_large,_bright,_white_room.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf57.83.100\n",
      "    Stream #0:0: Video: h264 (libx264) (avc1 / 0x31637661), yuv420p, 512x512 [SAR 1:1 DAR 1:1], q=-1--1, 1 fps, 16384 tbn, 1 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc57.107.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "\u001b[0;36m[mjpeg @ 0x55ddb6924040] \u001b[0m\u001b[0;33mEOI missing, emulating\n",
      "\u001b[1;35m[mp4 @ 0x55ddb64d6360] \u001b[0mStarting second pass: moving the moov atom to the beginning of the file\n",
      "frame=  100 fps=0.0 q=-1.0 Lsize=     745kB time=00:01:39.00 bitrate=  61.6kbits/s speed= 103x    \n",
      "video:743kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.213209%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mframe I:98    Avg QP: 7.30  size:  7611\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mframe P:2     Avg QP: 7.64  size:  7146\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mmb I  I16..4: 71.9%  0.0% 28.1%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mmb P  I16..4: 44.1%  0.0% 15.6%  P16..4: 12.4%  9.9%  3.3%  0.0%  0.0%    skip:14.7%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mcoded y,uvDC,uvAC intra: 33.9% 6.7% 2.5% inter: 19.0% 3.2% 0.1%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mi16 v,h,dc,p: 63% 29%  6%  2%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mi4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 38% 36% 14%  2%  2%  2%  3%  1%  2%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mi8c dc,h,v,p: 89%  6%  4%  0%\n",
      "\u001b[1;36m[libx264 @ 0x55ddb64d7580] \u001b[0mkb/s:60.81\n"
     ]
    }
   ],
   "source": [
    "# generate mp4\n",
    "out = '%s.mp4'%prompt.replace(\" \", \"_\")\n",
    "with open('list.txt','w') as f:\n",
    "  for i in range(sample_num):\n",
    "    f.write('file frame_%05d.jpg\\n'%i)\n",
    "  for j in range(int(freeze_secs*fps)):\n",
    "    f.write('file frame_%05d.jpg\\n'%i)\n",
    "!ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
    "\n",
    "# rename jpg\n",
    "frame = 'frame_%05d.jpg'%(sample_num-1)\n",
    "jpg = '%s.jpg'%prompt.replace(\" \", \"_\")\n",
    "!cp $frame $jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to datestamped path\n",
    "import os, datetime\n",
    "newdir = outpath[:-1]+\"_\"+datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "!mv $outpath $newdir\n",
    "!mkdir -p $outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Click here to download: <a href='a_buddha_sculpture_with_television_in_the_grass.mp4' target='_blank'>a_buddha_sculpture_with_television_in_the_grass.mp4</a><br>"
      ],
      "text/plain": [
       "/home/jovyan/work/output_20210201_022336/a_buddha_sculpture_with_television_in_the_grass.mp4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, FileLink, FileLinks\n",
    "\n",
    "local_file = FileLink(out.replace('\"', \"\"), result_html_prefix=\"Click here to download: \")\n",
    "# local_file = FileLinks(\".\", result_html_suffix=\"?download\")\n",
    "display(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"a_buddha_sculpture_with_television_in_the_grass.mp4\">download a_buddha_sculpture_with_television_in_the_grass.mp4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "local_file = out.replace('\"', \"\")\n",
    "HTML(\"<a href=\\\"\"+local_file+\"\\\">download %s</a>\"%local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WanderCLIP.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
