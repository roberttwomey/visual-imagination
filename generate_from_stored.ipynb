{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwTP4MYk0bYn"
   },
   "source": [
    "# Artificial Visual Imagination \n",
    "## Text to Image with BigGAN + CLIP + CMA-ES\n",
    "\n",
    "---\n",
    "\n",
    "BIGCLIP [j.mp/bigclip](https://j.mp/bigclip) by Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com)\n",
    "\n",
    "\n",
    "Modified to run on nautilus.optiputer.net by robert.twomey@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Generating from Saved Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from https://github.com/huggingface/pytorch-pretrained-BigGAN#usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup **run once** (until I fix my deployment with new jupyter stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.1+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp39-cp39-linux_x86_64.whl (735.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 735.4 MB 10 kB/s s eta 0:00:012   |▌                               | 12.5 MB 18.4 MB/s eta 0:00:40     |██████▎                         | 144.9 MB 90.1 MB/s eta 0:00:07     |████████▌                       | 194.5 MB 77.4 MB/s eta 0:00:07MB 100.9 MB/s eta 0:00:05��█▏                     | 232.7 MB 100.9 MB/s eta 0:00:05��█████▎                     | 237.2 MB 100.9 MB/s eta 0:00:05�████████▌                   | 287.9 MB 103.2 MB/s eta 0:00:05     |████████████▋                   | 291.0 MB 103.2 MB/s eta 0:00:05     |████████████▉                   | 294.1 MB 103.2 MB/s eta 0:00:05     |█████████████▍                  | 307.5 MB 103.2 MB/s eta 0:00:05     |█████████████▌                  | 309.4 MB 65.5 MB/s eta 0:00:07██████████                  | 319.4 MB 65.5 MB/s eta 0:00:07/s eta 0:00:06     |███████████████▏                | 349.2 MB 65.5 MB/s eta 0:00:06     |███████████████▎                | 350.9 MB 65.5 MB/s eta 0:00:06     |███████████████████▋            | 451.2 MB 96.6 MB/s eta 0:00:03     |█████████████████████▎          | 487.9 MB 46.9 MB/s eta 0:00:06     |█████████████████████▍          | 490.6 MB 46.9 MB/s eta 0:00:06     |███████████████████████▊        | 544.1 MB 46.9 MB/s eta 0:00:05     |███████████████████████▉        | 548.6 MB 116.6 MB/s eta 0:00:02     |█████████████████████████▋      | 588.8 MB 116.6 MB/s eta 0:00:02��███████▋     | 610.7 MB 116.6 MB/s eta 0:00:02     |██████████████████████████▋     | 612.3 MB 116.6 MB/s eta 0:00:02 | 612.7 MB 578 kB/s eta 0:03:33�████████▊     | 612.8 MB 578 kB/s eta 0:03:32 | 616.6 MB 739 kB/s eta 0:02:41�████████▉     | 616.8 MB 739 kB/s eta 0:02:41 | 617.4 MB 1.5 MB/s eta 0:01:22�████████▉     | 617.6 MB 514 kB/s eta 0:03:50███████████████████▉     | 617.7 MB 514 kB/s eta 0:03:499████████▉    | 640.7 MB 4.2 MB/s eta 0:00:23��███████████████████▉   | 661.6 MB 4.2 MB/s eta 0:00:18     |█████████████████████████████   | 667.5 MB 4.2 MB/s eta 0:00:178 MB 4.2 MB/s eta 0:00:09\n",
      "\u001b[?25hCollecting torchvision==0.8.2+cu101\n",
      "  Downloading https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp39-cp39-linux_x86_64.whl (12.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.7 MB 44.2 MB/s eta 0:00:01a 0:00:01 |███████████████████████████████ | 12.3 MB 44.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy\n",
      "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 3.5 MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting regex\n",
      "  Downloading regex-2021.8.28-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (759 kB)\n",
      "\u001b[K     |████████████████████████████████| 759 kB 32.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch==1.7.1+cu101) (3.7.4.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from torch==1.7.1+cu101) (1.19.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /opt/conda/lib/python3.9/site-packages (from torchvision==0.8.2+cu101) (8.3.1)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.9/site-packages (from ftfy) (0.2.5)\n",
      "Building wheels for collected packages: ftfy\n",
      "  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41913 sha256=81ee15569fccffda73f59026dade4db86293039630bc2e65e9efc6e4cc795020\n",
      "  Stored in directory: /home/jovyan/.cache/pip/wheels/3d/ee/4b/03a4e2e591ea56687aff999edc83827a2ace523baab75b8e41\n",
      "Successfully built ftfy\n",
      "Installing collected packages: torch, torchvision, regex, ftfy\n",
      "Successfully installed ftfy-6.0.3 regex-2021.8.28 torch-1.7.1+cu101 torchvision-0.8.2+cu101\n",
      "Collecting pytorch-pretrained-biggan\n",
      "  Downloading pytorch_pretrained_biggan-0.1.1-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from pytorch-pretrained-biggan) (4.61.2)\n",
      "Requirement already satisfied: torch>=0.4.1 in /opt/conda/lib/python3.9/site-packages (from pytorch-pretrained-biggan) (1.7.1+cu101)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.9/site-packages (from pytorch-pretrained-biggan) (2.26.0)\n",
      "Collecting boto3\n",
      "  Downloading boto3-1.18.41-py3-none-any.whl (131 kB)\n",
      "\u001b[K     |████████████████████████████████| 131 kB 22.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.9/site-packages (from pytorch-pretrained-biggan) (1.19.5)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.9/site-packages (from torch>=0.4.1->pytorch-pretrained-biggan) (3.7.4.3)\n",
      "Collecting s3transfer<0.6.0,>=0.5.0\n",
      "  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n",
      "\u001b[K     |████████████████████████████████| 79 kB 13.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting botocore<1.22.0,>=1.21.41\n",
      "  Downloading botocore-1.21.41-py3-none-any.whl (7.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.9 MB 49.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
      "  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.41->boto3->pytorch-pretrained-biggan) (2.8.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.9/site-packages (from botocore<1.22.0,>=1.21.41->boto3->pytorch-pretrained-biggan) (1.26.6)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.22.0,>=1.21.41->boto3->pytorch-pretrained-biggan) (1.15.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.9/site-packages (from requests->pytorch-pretrained-biggan) (2.0.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests->pytorch-pretrained-biggan) (3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests->pytorch-pretrained-biggan) (2021.5.30)\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-biggan\n",
      "Successfully installed boto3-1.18.41 botocore-1.21.41 jmespath-0.10.0 pytorch-pretrained-biggan-0.1.1 s3transfer-0.5.0\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 5.7 MB/s eta 0:00:01     |█████████▌                      | 430 kB 5.7 MB/s eta 0:00:01 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /opt/conda/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: regex in /opt/conda/lib/python3.9/site-packages (from nltk) (2021.8.28)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.9/site-packages (from nltk) (4.61.2)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Installing collected packages: nltk\n",
      "Successfully installed nltk-3.6.2\n",
      "Collecting cma\n",
      "  Downloading cma-3.1.0-py2.py3-none-any.whl (269 kB)\n",
      "\u001b[K     |████████████████████████████████| 269 kB 14.9 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: cma\n",
      "Successfully installed cma-3.1.0\n"
     ]
    }
   ],
   "source": [
    "# # !pip install perlin-noise\n",
    "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 -f https://download.pytorch.org/whl/torch_stable.html ftfy regex\n",
    "!pip install pytorch-pretrained-biggan\n",
    "!pip install nltk\n",
    "!pip install cma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipython-autotime\n",
      "  Downloading ipython_autotime-0.3.1-py2.py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from ipython-autotime) (7.14.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (2.6.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (4.4.2)\n",
      "Requirement already satisfied: jedi>=0.10 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.17.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (0.1.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (46.4.0.post20200518)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (3.0.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (4.3.3)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-autotime) (4.8.0)\n",
      "Requirement already satisfied: parso>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.10->ipython->ipython-autotime) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-autotime) (0.1.9)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->ipython-autotime) (0.2.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from traitlets>=4.2->ipython->ipython-autotime) (1.14.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect; sys_platform != \"win32\"->ipython->ipython-autotime) (0.6.0)\n",
      "Installing collected packages: ipython-autotime\n",
      "Successfully installed ipython-autotime-0.3.1\n",
      "time: 1.06 ms (started: 2021-09-15 14:36:14 +00:00)\n"
     ]
    }
   ],
   "source": [
    "!pip install ipython-autotime\n",
    "%load_ext autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "100%|██████████| 235773527/235773527 [00:55<00:00, 4285599.37B/s]\n",
      "100%|██████████| 800/800 [00:00<00:00, 335175.63B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded bigGAN\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import HTML, clear_output\n",
    "from PIL import Image\n",
    "from IPython.display import Image as JupImage\n",
    "import numpy as np\n",
    "import nltk\n",
    "from scipy.stats import truncnorm\n",
    "\n",
    "# from biggan\n",
    "import torch\n",
    "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
    "                                       save_as_images, convert_to_images) #, display_in_terminal)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# do we need wordnet?\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# load biggan\n",
    "model = BigGAN.from_pretrained('biggan-deep-512')\n",
    "print(\"loaded bigGAN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Latent+Class Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import vstack\n",
    "from numpy.random import randn\n",
    "from numpy.random import randint\n",
    "from numpy import arccos\n",
    "from numpy import clip\n",
    "from numpy import dot\n",
    "from numpy import sin\n",
    "from numpy import linspace\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helpers for interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from\n",
    "# https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/4\n",
    "\n",
    "# spherical linear interpolation (slerp)\n",
    "def slerp(val, low, high):\n",
    "    omega = arccos(clip(dot(low/norm(low), high/norm(high)), -1, 1))\n",
    "    so = sin(omega)\n",
    "    if so == 0:\n",
    "        # L'Hopital's rule/LERP\n",
    "        return (1.0-val) * low + val * high\n",
    "    return sin((1.0-val)*omega) / so * low + sin(val*omega) / so * high\n",
    " \n",
    "# uniform interpolation between two points in latent space\n",
    "def interpolate_points(p1, p2, n_steps=10):\n",
    "    # interpolate ratios between the points\n",
    "    ratios = np.linspace(0, 1, num=n_steps)\n",
    "    # linear interpolate vectors\n",
    "    vectors = list()\n",
    "    for ratio in ratios:\n",
    "        v = slerp(ratio, p1, p2)\n",
    "        vectors.append(v)\n",
    "    return np.asarray(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File output settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to seed the interpolation for generation from stored vectors\n",
    "# np.random.RandomState(1)\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# one random variable\n",
    "truncation = 1.0\n",
    "\n",
    "# the file directories\n",
    "workbase = '/home/jovyan/work/'\n",
    "\n",
    "# results\n",
    "resultsbase = \"/home/jovyan/work/results/results_pom_seed_128/\"\n",
    "\n",
    "# intermediate frames working directory\n",
    "interpbase = '/home/jovyan/work/interpolation/pom_interp_128'\n",
    "\n",
    "# video and srt output files\n",
    "filebase = 'pom_128_%s'\n",
    "moviefilename = filebase+'.mp4'\n",
    "srtfilename = filebase+'.srt'\n",
    "\n",
    "\n",
    "# the interpolation\n",
    "num_steps = 90#300\n",
    "len_hold = 30\n",
    "\n",
    "# the movie\n",
    "fps = 30"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!mkdir -p $interpbase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     # 001\n",
    "#     \"an elegant machine that learns to generate artificial images\",\n",
    "#     \"the sublime experience of an iceberg\",\n",
    "#     \"ice is a vastness of possibilities\",\n",
    "#     \"the vastness of possibilities of water is not a spatial vastness\",\n",
    "#     # 002\n",
    "#     \"a drawing of an elegant machine\",\n",
    "#     \"a machine that learns to make images\",\n",
    "#     \"a drawing of a machine that learns to make images\",\n",
    "#     # 003\n",
    "#     \"eighteenth century painting of humans encountering nature\",\n",
    "#     \"a person encountering nature\",\n",
    "#     \"a vastness of spatial dimensions\",\n",
    "#     \"a network\",\n",
    "#     \"a network of vast spatial dimensions\",\n",
    "#     \"a space of unlimited possibilities that the network must explore\"\n",
    "# ]\n",
    "\n",
    "# def make_safe_filename(s):\n",
    "#     def safe_char(c):\n",
    "#         if c.isalnum():\n",
    "#             return c\n",
    "#         else:\n",
    "#             return \"_\"\n",
    "#     return \"\".join(safe_char(c) for c in s).rstrip(\"_\")\n",
    "\n",
    "# def get_class_file(path, prompt):\n",
    "#     #print(path+'%s_class.txt'%prompt)\n",
    "#     result = glob.glob(path+'%s_class.txt'%prompt)\n",
    "#     return(result)\n",
    "\n",
    "# def get_noise_file(path, prompt):\n",
    "#     #print(path+'%s_noise.txt'%prompt)\n",
    "#     result = glob.glob(path+'%s_noise.txt'%prompt)\n",
    "#     return(result)\n",
    "\n",
    "# safe_prompts = [make_safe_filename(prompt) for prompt in prompts]\n",
    "\n",
    "# # print(safe_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_class_file(resultsbase, 'the_sublime_experience_of_an_iceberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# safe_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_filenames = [get_class_file(resultsbase, prompt)[0] for prompt in safe_prompts]\n",
    "# noise_filenames = [get_noise_file(resultsbase, prompt)[0] for prompt in safe_prompts]\n",
    "\n",
    "# # print(class_filenames, noise_filenames)\n",
    "# # print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "# noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "# # print(class_inputs, noise_inputs)\n",
    "# # print(class_inputs[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate from saved class and noise vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 80 350\n"
     ]
    }
   ],
   "source": [
    "# count = 0\n",
    "\n",
    "# for i in range(len(class_inputs)):\n",
    "\n",
    "#     # generate interpolations\n",
    "#     noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "#     classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "#     # generate images in batches\n",
    "#     batch_size = 10 #50 #RTX8000\n",
    "#     for j in range(0, num_steps, batch_size):\n",
    "#         clear_output()\n",
    "#         print(i, j, count)\n",
    "#         noise_vector = noises[j:j+batch_size]\n",
    "#         class_vector = classes[j:j+batch_size]\n",
    "\n",
    "#         # convert to tensors\n",
    "#         noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "#         class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "#         # put everything on cuda (GPU)\n",
    "#         noise_vector = noise_vector.to('cuda')\n",
    "#         noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "#         class_vector = class_vector.to('cuda')\n",
    "#         class_vector = class_vector.softmax(dim=-1)\n",
    "#         model.to('cuda')\n",
    "\n",
    "#         # generate images\n",
    "#         with torch.no_grad():\n",
    "#             output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "#         # If you have a GPU put back on CPU\n",
    "#         output = output.to('cpu')\n",
    "\n",
    "#         imgs = convert_to_images(output)\n",
    "\n",
    "#         # repeat first image\n",
    "        \n",
    "#         if j == 0:\n",
    "#             for k in range(len_hold):\n",
    "#                 imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
    "#                 count = count + 1\n",
    "                \n",
    "#         for img in imgs: \n",
    "#             img.save(interpbase+\"/output_%05d.png\" % count)\n",
    "#             count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NON-BATCH GENERATION\n",
    "\n",
    "# num_steps = 300\n",
    "# len_hold = 30\n",
    "\n",
    "# class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "# noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "# # print(class_inputs, noise_inputs)\n",
    "\n",
    "# count = 0\n",
    "\n",
    "# for i in range(len(class_inputs)):\n",
    "    \n",
    "#     # interpolate\n",
    "#     noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "#     classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "#     for j in range(num_steps):\n",
    "\n",
    "#         # expand dims\n",
    "#         noise_vector = np.expand_dims(noises[j], axis=0)\n",
    "#         class_vector = np.expand_dims(classes[j], axis=0)\n",
    "\n",
    "#         # convert to tensors\n",
    "#         noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "#         class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "    \n",
    "#         # If you have a GPU, put everything on cuda\n",
    "#         noise_vector = noise_vector.to('cuda')\n",
    "#         noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "#         class_vector = class_vector.to('cuda')\n",
    "#         class_vector = class_vector.softmax(dim=-1)\n",
    "#         model.to('cuda')\n",
    "\n",
    "#         # Generate an image\n",
    "#         with torch.no_grad():\n",
    "#             output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "#         # If you have a GPU put back on CPU\n",
    "#         output = output.to('cpu')\n",
    "\n",
    "#         # # If you have a sixtel compatible terminal you can display the images in the terminal\n",
    "#         # # (see https://github.com/saitoha/libsixel for details)\n",
    "    \n",
    "#         # \"hold\" on the first frame for fixed time\n",
    "#         if j == 0:\n",
    "#             for k in range(len_hold-1):\n",
    "#                 save_as_images(output, interpbase+\"/output_%05d\" % count)\n",
    "#                 count = count + 1\n",
    "#     #     clear_output()\n",
    "#     #     display(JupImage(\"output_%05d_0.png\" % i))\n",
    "    \n",
    "#         save_as_images(output, interpbase+\"/output_%05d\" % count)\n",
    "#         count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.2.4-1ubuntu0.1 Copyright (c) 2000-2020 the FFmpeg developers\n",
      "  built with gcc 9 (Ubuntu 9.3.0-10ubuntu2)\n",
      "  configuration: --prefix=/usr --extra-version=1ubuntu0.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-avresample --disable-filter=resample --enable-avisynth --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librsvg --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwavpack --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-nvenc --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 31.100 / 56. 31.100\n",
      "  libavcodec     58. 54.100 / 58. 54.100\n",
      "  libavformat    58. 29.100 / 58. 29.100\n",
      "  libavdevice    58.  8.100 / 58.  8.100\n",
      "  libavfilter     7. 57.100 /  7. 57.100\n",
      "  libavresample   4.  0.  0 /  4.  0.  0\n",
      "  libswscale      5.  5.100 /  5.  5.100\n",
      "  libswresample   3.  5.100 /  3.  5.100\n",
      "  libpostproc    55.  5.100 / 55.  5.100\n",
      "Input #0, image2, from '/home/jovyan/work/interpolation/pom_interp_128_hold_the_vastness_of_possibilities_of_water_is_not_a_spatial_vastness/output_%05d.png':\n",
      "  Duration: 00:00:14.40, start: 0.000000, bitrate: N/A\n",
      "    Stream #0:0: Video: png, rgb24(pc), 512x512, 25 fps, 25 tbr, 25 tbn, 25 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (png (native) -> hevc (libx265))\n",
      "Press [q] to stop, [?] for help\n",
      "x265 [info]: HEVC encoder version 3.2.1+1-b5c86a64bbbe\n",
      "x265 [info]: build info [Linux][GCC 9.3.0][64 bit] 8bit+10bit+12bit\n",
      "x265 [info]: using cpu capabilities: MMX2 SSE2Fast LZCNT SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2\n",
      "x265 [info]: Main profile, Level-3 (Main tier)\n",
      "x265 [info]: Thread pool 0 using 64 threads on numa nodes 0,1\n",
      "x265 [info]: Slices                              : 1\n",
      "x265 [info]: frame threads / pool features       : 5 / wpp(8 rows)\n",
      "x265 [warning]: Source height < 720p; disabling lookahead-slices\n",
      "x265 [info]: Coding QT: max CU size, min CU size : 64 / 8\n",
      "x265 [info]: Residual QT: max TU size, max depth : 32 / 1 inter / 1 intra\n",
      "x265 [info]: ME / range / subpel / merge         : hex / 57 / 2 / 3\n",
      "x265 [info]: Keyframe min / max / scenecut / bias: 25 / 250 / 40 / 5.00\n",
      "x265 [info]: Lookahead / bframes / badapt        : 20 / 4 / 2\n",
      "x265 [info]: b-pyramid / weightp / weightb       : 1 / 1 / 0\n",
      "x265 [info]: References / ref-limit  cu / depth  : 3 / off / on\n",
      "x265 [info]: AQ: mode / str / qg-size / cu-tree  : 2 / 1.0 / 32 / 1\n",
      "x265 [info]: Rate Control / qCompress            : CRF-0.0 / 0.60\n",
      "x265 [info]: tools: rd=3 psy-rd=2.00 early-skip rskip signhide tmvp b-intra\n",
      "x265 [info]: tools: strong-intra-smoothing deblock sao\n",
      "Output #0, mp4, to '/home/jovyan/work/interpolation/pom_128_30_hold_the_vastness_of_possibilities_of_water_is_not_a_spatial_vastness.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.29.100\n",
      "    Stream #0:0: Video: hevc (libx265) (hev1 / 0x31766568), yuv420p, 512x512, q=2-31, 30 fps, 15360 tbn, 30 tbc\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.54.100 libx265\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: -1\n",
      "frame=  360 fps= 15 q=-0.0 Lsize=   41114kB time=00:00:11.90 bitrate=28303.0kbits/s speed=0.496x    \n",
      "video:41109kB audio:0kB subtitle:0kB other streams:0kB global headers:2kB muxing overhead: 0.012726%\n",
      "x265 [info]: frame I:      2, Avg QP:0.33  kb/s: 50271.72\n",
      "x265 [info]: frame P:    343, Avg QP:0.59  kb/s: 28688.60\n",
      "x265 [info]: frame B:     15, Avg QP:6.89  kb/s: 10790.72\n",
      "x265 [info]: Weighted P-Frames: Y:0.0% UV:0.0%\n",
      "x265 [info]: consecutive B-frames: 98.8% 0.0% 0.0% 0.3% 0.9% \n",
      "\n",
      "encoded 360 frames in 23.95s (15.03 fps), 28062.76 kb/s, Avg QP:0.85\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # generate mp4\n",
    "# out = moviefilename%fps\n",
    "# with open('list.txt','w') as f:\n",
    "#   for i in range(count):\n",
    "# #     print('file %s/output_%05d.png\\n'%(interpbase, i))\n",
    "#     f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
    "# # !ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
    "# # !echo ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
    "# # cmd = \"ffmpeg -r {0} -i list.txt -c:v libx264rgb -crf 0 -r {0} {1} -y\"\n",
    "# cmd = \"ffmpeg -r {0} -i {1}/output_%05d.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
    "        \n",
    "# # os.system(\"ffmpeg -r {0} -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r {0} {1} -y\".format(fps, out))\n",
    "# os.system(cmd.format(fps, interpbase, out))\n",
    "\n",
    "# # # # rename jpg\n",
    "# # # frame = 'frame_%05d.jpg'%(sample_num-1)\n",
    "# # # jpg = '%s.jpg'%prompt.replace(\" \", \"_\")\n",
    "# # # !cp $frame $jpg\n",
    "# # print(\"ffmpeg -r {0} -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r {0} {1} -y\".format(fps, out))\n",
    "# # print(cmd.format(fps, interpbase, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1226/3490683107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mlen_hold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes_to_TC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# def frames_to_TC (frames):\n",
    "#     h = int(frames / 86400) \n",
    "#     m = int(frames / (60*fps)) % 60 \n",
    "#     s = int((frames % (60*fps))/fps) \n",
    "#     f = float(frames % (60*fps) % fps)/float(fps)\n",
    "#     return ( \"%02d:%02d:%02d,%04d\" % ( h, m, s, f))\n",
    "\n",
    "# with open(srtfilename%fps,'w') as f:\n",
    "\n",
    "# #     count = 0\n",
    "#     frames = 0\n",
    "#     for i in range(len(class_inputs)):\n",
    "#         f.write(str(i+1)+\"\\n\")\n",
    "#         f.write(frames_to_TC(frames)+\" --> \")\n",
    "#         frames+=len_hold\n",
    "#         f.write(frames_to_TC(frames)+\"\\n\")\n",
    "#         f.write(prompts[i]+\"\\n\\n\")\n",
    "#         frames+=num_steps\n",
    "# print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Video Holding on Single Phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper for generating time codes for subtitles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_TC (frames):\n",
    "    h = int(frames / 86400) \n",
    "    m = int(frames / (60*fps)) % 60 \n",
    "    s = int((frames % (60*fps))/fps) \n",
    "    f = float(frames % (60*fps) % fps)/float(fps)\n",
    "    return ( \"%02d:%02d:%02d,%04d\" % ( h, m, s, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe_filename(s):\n",
    "    def safe_char(c):\n",
    "        if c.isalnum():\n",
    "            return c\n",
    "        else:\n",
    "            return \"_\"\n",
    "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video_hold(output_folder, class_filenames, result_filename, prompt, interpbase, filebase):\n",
    "    \"\"\"includes the result with the class_files\"\"\"\n",
    "    \n",
    "    print(\"==== retrieve vectors ====\")\n",
    "    \n",
    "    safe_prompt = make_safe_filename(prompt)\n",
    "\n",
    "    prompts = [ prompt*(len(class_filenames)+1)]\n",
    "\n",
    "    class_filenames = [output_folder+file for file in class_filenames]\n",
    "    class_filenames += [result_filename.format(safe_prompt)]\n",
    "\n",
    "    noise_filenames = [fname.replace(\"class\", \"noise\") for fname in class_filenames]\n",
    "\n",
    "    print(class_filenames)\n",
    "    print(noise_filenames)\n",
    "\n",
    "    class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "    noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "\n",
    "    interpbase = interpbase.format(safe_prompt)\n",
    "    filebase = filebase.format(safe_prompt)\n",
    "    moviefilename = filebase+'.mp4'\n",
    "    srtfilename = filebase+'.srt'\n",
    "\n",
    "    !mkdir -p $interpbase\n",
    "    \n",
    "    print(\"==== generate interpolations ====\")\n",
    "\n",
    "    count = 0\n",
    "    \n",
    "    num_inputs = len(class_inputs)\n",
    "    for i in range(len(class_inputs)):\n",
    "\n",
    "        # generate interpolations\n",
    "        noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "        classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "        # generate images in batches\n",
    "#         batch_size = 10 #2080\n",
    "        batch_size = 60 #RTX8000\n",
    "        for j in range(0, num_steps, batch_size):\n",
    "            clear_output()\n",
    "            print(\"{0}/{1} clases\\t{2}/{3} steps\\t{4} frames\".format(i, num_inputs, j, num_steps, count))\n",
    "            noise_vector = noises[j:j+batch_size]\n",
    "            class_vector = classes[j:j+batch_size]\n",
    "\n",
    "            # convert to tensors\n",
    "            noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "            class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "            # put everything on cuda (GPU)\n",
    "            noise_vector = noise_vector.to('cuda')\n",
    "            noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "            class_vector = class_vector.to('cuda')\n",
    "            class_vector = class_vector.softmax(dim=-1)\n",
    "            model.to('cuda')\n",
    "\n",
    "            # generate images\n",
    "            with torch.no_grad():\n",
    "                output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "            # If you have a GPU put back on CPU\n",
    "            output = output.to('cpu')\n",
    "\n",
    "            imgs = convert_to_images(output)\n",
    "\n",
    "            # repeat first image\n",
    "\n",
    "            if j == 0:\n",
    "                for k in range(len_hold):\n",
    "                    imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
    "                    count = count + 1\n",
    "\n",
    "            for img in imgs: \n",
    "                img.save(interpbase+\"/output_%05d.png\" % count)\n",
    "                count = count + 1\n",
    "    \n",
    "    \n",
    "    print(\"==== generating movie ====\")\n",
    "    \n",
    "    # generate mp4\n",
    "    out = moviefilename%fps\n",
    "    with open('list.txt','w') as f:\n",
    "      for i in range(count):\n",
    "        f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
    "    cmd = \"ffmpeg -r {0} -i {1}/output_%05d.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
    "\n",
    "    os.system(cmd.format(fps, interpbase, out))\n",
    "    print(cmd.format(fps, interpbase, out))\n",
    "    \n",
    "    print(\"==== generating subtitles ==== \")\n",
    "    \n",
    "    with open(srtfilename%fps,'w') as f:\n",
    "\n",
    "        frames = 0\n",
    "        for i in range(len(class_inputs)):\n",
    "            f.write(str(i+1)+\"\\n\")\n",
    "            f.write(frames_to_TC(frames)+\" --> \")\n",
    "            frames+=len_hold\n",
    "            f.write(frames_to_TC(frames)+\"\\n\")\n",
    "            f.write(prompt+\"\\n\\n\")\n",
    "            frames+=num_steps\n",
    "    print(\"subtitles: \"+srtfilename%fps)\n",
    "    print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/3 clases\t60/90 steps\t330 frames\n",
      "==== generating movie ====\n",
      "==== generating subtitles ==== \n",
      "360\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_drawing_of_a_machine_that_learns_to_make_images*\" .\n",
      "time: 54.3 s (started: 2021-09-15 18:58:34 +00:00)\n"
     ]
    }
   ],
   "source": [
    "resultsbase = \"/home/jovyan/work/results/results_pom_seed_128/\"\n",
    "\n",
    "# video and srt output files\n",
    "filebase = 'pom_128_%s'\n",
    "# filebase = 'pom_10_%s'\n",
    "moviefilename = filebase+'.mp4'\n",
    "srtfilename = filebase+'.srt'\n",
    "\n",
    "#     \"they make us have arms and not wings\",    \n",
    "#     \"in these containers, we care for machines\",\n",
    "\n",
    "#     # prompt 2\n",
    "#     \"a family of robotic seals\",\n",
    "#     \"nursing homes in japan\",\n",
    "#     \"to help the elderly cope\", \n",
    "#     \"the after effects of the Tsunami\",\n",
    "#     \"the children laugh and smile\",\n",
    "#     \"their homes washed away\"\n",
    "#     \"watching the news on television\",\n",
    "#     \"spoke through a microphone\",\n",
    "#     \"the robotic seals danced\"\n",
    "#     \"the glow of a television screen\",\n",
    "    \n",
    "#     # prompt 3\n",
    "#     \"an xray of the first silicon chip\",\n",
    "#     \"the dover demonstration chip\",\n",
    "#     \"a picture of his arthritic hands\",\n",
    "#     \"a piece of wire to connect two silicon squares\",\n",
    "#     \"smaller and smaller computer chips\",\n",
    "#     \"the kilby diode\",\n",
    "#     \"a silicon mold of a friends hands\"\n",
    "\n",
    "#     #prompt 4\n",
    "#     \"the first computing devices were powered by steam engines\",\n",
    "#     \"fueled with the burning of fossilized coal\",\n",
    "#     \"the bones of sparse data structures\",\n",
    "#     \"the slow demise of our geological resources\",\n",
    "#     \"extracting lithium\",\n",
    "#     \"battery technology\",\n",
    "#     \"computing devices can be charged without being plugged in\",\n",
    "#     \"key contributor to climate change\",\n",
    "#     \"we are powered by imagination\",\n",
    "#     \"the machines we use to think\",\n",
    "#     \"the machines that are destroying the planet\"\n",
    "\n",
    "# prompts = [\n",
    "#     #prompt 5\n",
    "#     \"a painting of pygmalion\",\n",
    "#     \"a sculptor who fell in love with his own sculpture\",\n",
    "#     \"everyone who saw her would fall in love with her\",\n",
    "#     \"beautiful reflections of our digital twinse\",\n",
    "#     \"a statue of a woman\",\n",
    "#     \"a living breathing woman\",\n",
    "#     \"our digital twins, artificial intelligence\",\n",
    "#     \"a projection of our deepest fears\"\n",
    "# ]\n",
    "\n",
    "# prompts = [\"beautiful reflections of our digital twins\"]\n",
    "\n",
    "prompts = [\n",
    "#     \"the sublime experience of an iceberg\",\n",
    "#     \"ice is a vastness of possibilities\",\n",
    "#     \"a vastness of spatial dimensions\",\n",
    "#     \"a space of unlimited possibilities that the network must explore\",\n",
    "#     \"a network\",\n",
    "#     \"a network of vast spatial dimensions\",\n",
    "#     \"eighteenth century painting of humans encountering nature\",\n",
    "#     \"a person encountering nature\",\n",
    "#     \"an elegant machine that learns to generate artificial images\",\n",
    "#     \"a drawing of an elegant machine\",\n",
    "#     \"a machine that learns to make images\",\n",
    "    \"a drawing of a machine that learns to make images\",\n",
    "]\n",
    "\n",
    "for prompt in prompts:\n",
    "    \n",
    "    # prompt = \"a silicon mold of a friends hands\"\n",
    "\n",
    "    safe_prompt = make_safe_filename(prompt)\n",
    "\n",
    "    class_filenames = [ \n",
    "        \"class_00009.txt\", \n",
    "    #     \"class_00008.txt\", \n",
    "        \"class_00007.txt\"\n",
    "    ]\n",
    "\n",
    "    result_filename = resultsbase+\"{0}_class.txt\"\n",
    "\n",
    "    # stored output from synthesis\n",
    "    # safe_filename = make_safe_filename(prompt)\n",
    "\n",
    "    stored_output_folder = \"/home/jovyan/work/process/stored_outputs_pom_seed_128/\"+safe_prompt+\"/\"\n",
    "\n",
    "    # stored_output_folder = \"/home/jovyan/work/process/stored_outputs_pom_seed_128/\"+\\\n",
    "    #     \"a_mushrooms_above_ground_growth_is_the_result_of_a_set_of_instructions_20210914_214118/\"\n",
    "\n",
    "    interpbase = '/home/jovyan/work/interpolation/pom_interp_128_hold_{0}'\n",
    "    filebase = '/home/jovyan/work/interpolation/pom_128_%s_hold_{0}'\n",
    "\n",
    "    generate_video_hold(stored_output_folder, class_filenames, result_filename, prompt, interpbase, filebase)\n",
    "    print('scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_{0}*\" .'.format(safe_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workaround to download files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_the_sublime_experience_of_an_iceberg*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_ice_is_a_vastness_of_possibilities*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_vastness_of_spatial_dimensions*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_space_of_unlimited_possibilities_that_the_network_must_explore*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_network*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_network_of_vast_spatial_dimensionseighteenth_century_painting_of_humans_encountering_nature*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_person_encountering_nature*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_an_elegant_machine_that_learns_to_generate_artificial_images*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_drawing_of_an_elegant_machine*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_machine_that_learns_to_make_images*\" .\n",
      "scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_a_drawing_of_a_machine_that_learns_to_make_images*\" .\n",
      "time: 4.14 ms (started: 2021-09-15 18:59:31 +00:00)\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"the sublime experience of an iceberg\",\n",
    "    \"ice is a vastness of possibilities\",\n",
    "    \"a vastness of spatial dimensions\",\n",
    "    \"a space of unlimited possibilities that the network must explore\",\n",
    "    \"a network\",\n",
    "    \"a network of vast spatial dimensions\"\n",
    "    \"eighteenth century painting of humans encountering nature\",\n",
    "    \"a person encountering nature\",\n",
    "    \"an elegant machine that learns to generate artificial images\",\n",
    "    \"a drawing of an elegant machine\",\n",
    "    \"a machine that learns to make images\",\n",
    "    \"a drawing of a machine that learns to make images\",\n",
    "]\n",
    "for prompt in prompts:\n",
    "    \n",
    "    # prompt = \"a silicon mold of a friends hands\"\n",
    "\n",
    "    safe_prompt = make_safe_filename(prompt)\n",
    "    print('scp \"user@z8.local:/home/user/work/interpolation/pom_128_30_hold_{0}*\" .'.format(safe_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_to_TC (frames):\n",
    "    h = int(frames / 86400) \n",
    "    m = int(frames / (60*fps)) % 60 \n",
    "    s = int((frames % (60*fps))/fps) \n",
    "    f = float(frames % (60*fps) % fps)/float(fps)\n",
    "    return ( \"%02d:%02d:%02d,%04d\" % ( h, m, s, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_safe_filename(s):\n",
    "    def safe_char(c):\n",
    "        if c.isalnum():\n",
    "            return c\n",
    "        else:\n",
    "            return \"_\"\n",
    "    return \"\".join(safe_char(c) for c in s).rstrip(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.31 ms (started: 2021-09-14 22:46:26 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def generate_video_transition(class_filenames, prompts, interpbase):\n",
    "    \"\"\"includes the result with the class_files\"\"\"\n",
    "    \n",
    "    print(\"==== retrieve vectors ====\")    \n",
    "    safe_prompt = make_safe_filename(prompts[0])\n",
    "    noise_filenames = [fname.replace(\"class\", \"noise\") for fname in class_filenames]\n",
    "\n",
    "    print(class_filenames)\n",
    "    print(noise_filenames)\n",
    "\n",
    "    class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "    noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "\n",
    "    interpbase = interpbase.format(safe_prompt)\n",
    "    filebase = filebase.format(safe_prompt)\n",
    "    moviefilename = filebase+'.mp4'\n",
    "    srtfilename = filebase+'.srt'\n",
    "\n",
    "    !mkdir -p $interpbase\n",
    "    \n",
    "    print(\"==== generate interpolations ====\")\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    for i in range(len(class_inputs)):\n",
    "\n",
    "        # generate interpolations\n",
    "        noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "        classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "        # generate images in batches\n",
    "#         batch_size = 10\n",
    "        batch_size = 60 #50 #RTX8000\n",
    "    \n",
    "        for j in range(0, num_steps, batch_size):\n",
    "            clear_output()\n",
    "            print(i, j, count)\n",
    "            noise_vector = noises[j:j+batch_size]\n",
    "            class_vector = classes[j:j+batch_size]\n",
    "\n",
    "            # convert to tensors\n",
    "            noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "            class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "            # put everything on cuda (GPU)\n",
    "            noise_vector = noise_vector.to('cuda')\n",
    "            noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "            class_vector = class_vector.to('cuda')\n",
    "            class_vector = class_vector.softmax(dim=-1)\n",
    "            model.to('cuda')\n",
    "\n",
    "            # generate images\n",
    "            with torch.no_grad():\n",
    "                output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "            # If you have a GPU put back on CPU\n",
    "            output = output.to('cpu')\n",
    "\n",
    "            imgs = convert_to_images(output)\n",
    "\n",
    "            # repeat first image\n",
    "\n",
    "            if j == 0:\n",
    "                for k in range(len_hold):\n",
    "                    imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
    "                    count = count + 1\n",
    "\n",
    "            for img in imgs: \n",
    "                img.save(interpbase+\"/output_%05d.png\" % count)\n",
    "                count = count + 1\n",
    "    \n",
    "    \n",
    "    print(\"==== generating movie ====\")\n",
    "    \n",
    "    # generate mp4\n",
    "    out = moviefilename%fps\n",
    "    with open('list.txt','w') as f:\n",
    "      for i in range(count):\n",
    "        f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
    "    cmd = \"ffmpeg -r {0} -i {1}/output_%05d.png -c:v libx265 -pix_fmt yuv420p -crf 0 -r {0} {2} -y\"\n",
    "\n",
    "    os.system(cmd.format(fps, interpbase, out))\n",
    "    \n",
    "    print(\"==== generating subtitles ==== \")\n",
    "    \n",
    "    with open(srtfilename%fps,'w') as f:\n",
    "\n",
    "        frames = 0\n",
    "        for i in range(len(class_inputs)):\n",
    "            f.write(str(i+1)+\"\\n\")\n",
    "            f.write(frames_to_TC(frames)+\" --> \")\n",
    "            frames+=len_hold\n",
    "            f.write(frames_to_TC(frames)+\"\\n\")\n",
    "            f.write(prompt+\"\\n\\n\")\n",
    "            frames+=num_steps\n",
    "    print(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 60 210\n",
      "==== generating movie ====\n",
      "==== generating subtitles ==== \n",
      "240\n",
      "time: 41.5 s (started: 2021-09-14 22:46:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "resultsbase = \"/home/jovyan/work/results/results_pom_seed_128/\"\n",
    "\n",
    "# # video and srt output files\n",
    "# filebase = 'pom_128_%s'\n",
    "# # filebase = 'pom_10_%s'\n",
    "# moviefilename = filebase+'.mp4'\n",
    "# srtfilename = filebase+'.srt'\n",
    "\n",
    "prompts = [\n",
    "    \"an elegant machine that learns to generate artificial images\",\n",
    "    \"a machine that learns to make images\"\n",
    "]\n",
    "\n",
    "endpoints = [\n",
    "    \"an_elegant_machine_that_learns_to_generate_artificial_images_class.txt\",\n",
    "    \"a_machine_that_learns_to_make_images_class.txt\"\n",
    "]\n",
    "\n",
    "class_filenames = [resultsbase+endpoint for endpoint in endpoints]\n",
    "\n",
    "interpbase = '/home/jovyan/work/interpolation/pom_interp_128_{0}'\n",
    "    \n",
    "generate_video_transition(class_filenames, prompts, interpbase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # move to datestamped path\n",
    "# import os, datetime\n",
    "# newdir = outpath[:-1]+\"_\"+datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "# !mv $outpath $newdir\n",
    "# !mkdir -p $outpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import display, FileLink, FileLinks\n",
    "\n",
    "# local_file = FileLink(out.replace('\"', \"\"), result_html_prefix=\"Click here to download: \")\n",
    "# # local_file = FileLinks(\".\", result_html_suffix=\"?download\")\n",
    "# display(local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import HTML\n",
    "# local_file = out.replace('\"', \"\")\n",
    "# HTML(\"<a href=\\\"\"+local_file+\"\\\">download %s</a>\"%local_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolate with Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for perlin noise\n",
    "# import matplotlib.pyplot as plt\n",
    "# from perlin_noise import PerlinNoise\n",
    "\n",
    "# noise = PerlinNoise(octaves=10, seed=1)\n",
    "# xpix, ypix = noise_vector.shape[0], noise_vector.shape[1]\n",
    "\n",
    "# pic = [[noise([i/xpix, j/ypix]) for j in range(xpix)] for i in range(ypix)]\n",
    "\n",
    "# plt.imshow(pic, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# import math\n",
    "# from PIL import Image\n",
    "\n",
    "# perm = list(range(256))\n",
    "# random.shuffle(perm)\n",
    "# perm += perm\n",
    "# dirs = [(math.cos(a * 2.0 * math.pi / 256),\n",
    "#          math.sin(a * 2.0 * math.pi / 256))\n",
    "#          for a in range(256)]\n",
    "\n",
    "# def noise(x, y, per):\n",
    "#     def surflet(gridX, gridY):\n",
    "#         distX, distY = abs(x-gridX), abs(y-gridY)\n",
    "#         polyX = 1 - 6*distX**5 + 15*distX**4 - 10*distX**3\n",
    "#         polyY = 1 - 6*distY**5 + 15*distY**4 - 10*distY**3\n",
    "#         hashed = perm[perm[int(gridX)%per] + int(gridY)%per]\n",
    "#         grad = (x-gridX)*dirs[hashed][0] + (y-gridY)*dirs[hashed][1]\n",
    "#         return polyX * polyY * grad\n",
    "#     intX, intY = int(x), int(y)\n",
    "#     return (surflet(intX+0, intY+0) + surflet(intX+1, intY+0) +\n",
    "#             surflet(intX+0, intY+1) + surflet(intX+1, intY+1))\n",
    "\n",
    "# def fBm(x, y, per, octs):\n",
    "#     val = 0\n",
    "#     for o in range(octs):\n",
    "#         val += 0.5**o * noise(x*2**o, y*2**o, per*2**o)\n",
    "#     return val\n",
    "\n",
    "# size, freq, octs, data = 256, 1/32.0, 5, []\n",
    "# for y in range(size):\n",
    "#     for x in range(size):\n",
    "#         data.append(fBm(x*freq, y*freq, int(size*freq), octs))\n",
    "# data = np.array(data).reshape(size, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from skimage.transform import resize\n",
    "# img = resize(data, noise_vector.T.shape)\n",
    "# # img = data\n",
    "# plt.imshow(img, cmap='gray')\n",
    "# # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# noisebase = \"/home/jovyan/work/noise_interp\"\n",
    "# !mkdir -p $noisebase\n",
    "\n",
    "# num_steps = 100\n",
    "# noise_noise_scale = 1.0#0.1\n",
    "# class_noise_scale = 0.0\n",
    "# count = 0\n",
    "\n",
    "# i=0\n",
    "# # files\n",
    "# class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
    "# noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
    "\n",
    "# # interpolate\n",
    "# noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "# classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
    "\n",
    "# # generate vectors\n",
    "# batch_size = 10\n",
    "# for j in range(0, num_steps, batch_size):\n",
    "#     noise_vector = noises[j:j+batch_size]\n",
    "#     noise_noise = np.random.uniform(size=noise_vector.shape)*noise_noise_scale\n",
    "#     noise_vector = noise_vector + noise_noise\n",
    "\n",
    "#     class_vector = classes[j:j+batch_size]\n",
    "#     class_noise = np.random.uniform(size=class_vector.shape)*class_noise_scale\n",
    "#     class_vector = class_vector + class_noise\n",
    "f\n",
    "#     # expand dims\n",
    "# #     noise_vector = np.expand_dims(noise_vector, axis=0)\n",
    "# #     class_vector = np.expand_dims(class_vector, axis=0)\n",
    "\n",
    "#     # convert to tensors\n",
    "#     noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "#     class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "#     # If you have a GPU, put everything on cuda\n",
    "#     noise_vector = noise_vector.to('cuda')\n",
    "#     noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "#     class_vector = class_vector.to('cuda')\n",
    "#     class_vector = class_vector.softmax(dim=-1)\n",
    "#     model.to('cuda')\n",
    "\n",
    "#     # Generate an image\n",
    "#     with torch.no_grad():\n",
    "#         output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "#     # If you have a GPU put back on CPU\n",
    "#     output = output.to('cpu')\n",
    "\n",
    "# #     for k in range(output.shape[0]):\n",
    "# #         img = torch.unsqueeze(output[k], 0)\n",
    "# #         save_as_images(img, noisebase+\"/output_%05d\" % count)\n",
    "# #         # use convert_to_images instead\n",
    "# #         # see https://github.com/huggingface/pytorch-pretrained-BigGAN/blob/1e18aed2dff75db51428f13b940c38b923eb4a3d/pytorch_pretrained_biggan/utils.py#L36\n",
    "# #         count = count + 1\n",
    "#     imgs = convert_to_images(output)\n",
    "#     for img in imgs: \n",
    "#         img.save(noisebase+\"/output_%05d.png\" % count)\n",
    "#         count = count + 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Render as video (mp4) with ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fps = 30\n",
    "\n",
    "# out = 'domestic_class_noise%s_%s_%s.mp4'% (fps, noise_noise_scale, class_noise_scale)\n",
    "# with open('list.txt','w') as f:\n",
    "#   for i in range(count):\n",
    "#     f.write('file %snoise_interp/output_%05d.png\\n'%(workbase, i))\n",
    "# !ffmpeg -loglevel quiet -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
    "# # !echo ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\n",
    "\n",
    "Other CLIP notebooks: [OpenAI tutorial](https://colab.research.google.com/github/openai/clip/blob/master/Interacting_with_CLIP.ipynb), [SIREN by @advadnoun](https://colab.research.google.com/drive/1FoHdqoqKntliaQKnMoNs3yn5EALqWtvP), [SIREN by @norod78](https://colab.research.google.com/drive/1K1vfpTEvAmxW2rnhAaALRVyis8EiLOnD), [BigGAN by @advadnoun](https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR), [BigGAN by @eyaler](j.mp/bigclip), [BigGAN by @tg_bomze](https://colab.research.google.com/github/tg-bomze/collection-of-notebooks/blob/master/Text2Image_v2.ipynb), [BigGAN using big-sleep library by @lucidrains](https://colab.research.google.com/drive/1MEWKbm-driRNF8PrU7ogS5o3se-ePyPb), [BigGAN story hallucinator by @bonkerfield](https://colab.research.google.com/drive/1jF8pyZ7uaNYbk9ZiVdxTOajkp8kbmkLK), [StyleGAN2-ADA Anime by @nagolinc](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/TADNE_and_CLIP.ipynb) [v2](https://colab.research.google.com/github/nagolinc/notebooks/blob/main/CLIP_%2B_TADNE_(pytorch)_v2.ipynb)\n",
    "\n",
    "Using the works:\n",
    "\n",
    "https://github.com/openai/CLIP\n",
    "\n",
    "https://tfhub.dev/deepmind/biggan-deep-512\n",
    "\n",
    "https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
    "\n",
    "http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\n",
    "\n",
    "For a curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leftovers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex2. Interpolating between two vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.RandomState(1)\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "# num_steps = 500\n",
    "\n",
    "# # Prepare a input\n",
    "# truncation = 1.0\n",
    "# workbase = '/home/jovyan/'\n",
    "\n",
    "# # sculptures: \n",
    "\n",
    "\n",
    "# # work/visual-imagination/CLIP/class_sculpture television buddha.txt\n",
    "# # work/visual-imagination/CLIP/class_a buddha sculpture with television in the grass.txt\n",
    "# # work/visual-imagination/CLIP/class_television buddha sculpture with grass.txt\n",
    "# # work/visual-imagination/CLIP/class_old television in grass with buddha sculpture by pacific ocean.txt\n",
    "# # work/visual-imagination/CLIP/class_old television in long grass with buddha sculpture by pacific ocean.txt\n",
    "\n",
    "\n",
    "# noise1 = np.loadtxt(workbase+'work/output_20210222_195143/noise_00002.txt')\n",
    "# class1 = np.loadtxt(workbase+'work/output_20210222_195143/class_00002.txt')\n",
    "\n",
    "# noise2 = np.loadtxt(workbase+'work/output_20210222_195143/noise_00008.txt')\n",
    "# class2 = np.loadtxt(workbase+'work/output_20210222_195143/class_00008.txt')\n",
    "\n",
    "# # noise_vector = np.loadtxt(workbase+'work/visual-imagination/CLIP/noise_a room with good lighting.txt')\n",
    "# # class_vector = np.loadtxt(workbase+'work/visual-imagination/CLIP/class_a room with good lighting.txt')\n",
    "\n",
    "# # interpolate\n",
    "# noises = interpolate_points(noise1, noise2, num_steps)\n",
    "# classes = interpolate_points(class1, class2, num_steps)\n",
    "\n",
    "# # expand dims (only necessary for single vector)\n",
    "\n",
    "# for i in range(num_steps):\n",
    "    \n",
    "#     # expand dims\n",
    "#     noise_vector = np.expand_dims(noises[i], axis=0)\n",
    "#     class_vector = np.expand_dims(classes[i], axis=0)\n",
    "\n",
    "#     # convert to tensors\n",
    "#     noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "#     class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "#     print(noise_vector.shape, noise_vector.dtype, class_vector.shape, class_vector.dtype)\n",
    "\n",
    "#     # If you have a GPU, put everything on cuda\n",
    "#     noise_vector = noise_vector.to('cuda')\n",
    "#     noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "#     class_vector = class_vector.to('cuda')\n",
    "#     class_vector = class_vector.softmax(dim=-1)\n",
    "#     model.to('cuda')\n",
    "\n",
    "#     # Generate an image\n",
    "#     with torch.no_grad():\n",
    "#         output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "#     # If you have a GPU put back on CPU\n",
    "#     output = output.to('cpu')\n",
    "\n",
    "#     # # If you have a sixtel compatible terminal you can display the images in the terminal\n",
    "#     # # (see https://github.com/saitoha/libsixel for details)\n",
    "#     # # display_in_terminal(output)\n",
    "#     save_as_images(output, workbase+\"work/output_%05d\" % i)\n",
    "# #     clear_output()\n",
    "# #     display(JupImage(\"output_%05d_0.png\" % i))\n",
    "#     i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ex.1: Generate from a single stored noise/class vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # random seeding\n",
    "# np.random.RandomState(1)\n",
    "# np.random.seed(1)\n",
    "# torch.manual_seed(1)\n",
    "\n",
    "\n",
    "# # Prepare a input\n",
    "# truncation = 1.0\n",
    "# workbase = '/home/jovyan/'\n",
    "# noise_vector = np.loadtxt(workbase+'work/results/That_never_left_any_alive_who_stayed_in_it._noise.txt')\n",
    "# class_vector = np.loadtxt(workbase+'work/results/That_never_left_any_alive_who_stayed_in_it._class.txt')\n",
    "\n",
    "# # noise_vector = np.loadtxt(workbase+'work/visual-imagination/CLIP/noise_a room with good lighting.txt')\n",
    "# # class_vector = np.loadtxt(workbase+'work/visual-imagination/CLIP/class_a room with good lighting.txt')\n",
    "\n",
    "# # noise_vector = noise_vectors\n",
    "# # class_vector = class_vectors\n",
    "\n",
    "# # expand dims\n",
    "# noise_vector = np.expand_dims(noise_vector, axis=0)\n",
    "# class_vector = np.expand_dims(class_vector, axis=0)\n",
    "\n",
    "# # All in tensors\n",
    "# noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
    "# class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
    "\n",
    "# print(noise_vector.shape, noise_vector.dtype, class_vector.shape, class_vector.dtype)\n",
    "\n",
    "# # If you have a GPU, put everything on cuda\n",
    "# noise_vector = noise_vector.to('cuda')\n",
    "# noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
    "# class_vector = class_vector.to('cuda')\n",
    "# class_vector = class_vector.softmax(dim=-1)\n",
    "# model.to('cuda')\n",
    "\n",
    "# # Generate an image\n",
    "# with torch.no_grad():\n",
    "#     output = model(noise_vector, class_vector, truncation)\n",
    "\n",
    "# # If you have a GPU put back on CPU\n",
    "# output = output.to('cpu')\n",
    "\n",
    "# # # If you have a sixtel compatible terminal you can display the images in the terminal\n",
    "# # # (see https://github.com/saitoha/libsixel for details)\n",
    "# # # display_in_terminal(output)\n",
    "# # save_as_images(output, \"output_%s\" % index)\n",
    "\n",
    "# # display(Image(\"output_0.png\"))\n",
    "\n",
    "# # # Save results as png images\n",
    "# save_as_images(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sculptures: \n",
    "\n",
    "# class_filenames = [\n",
    "# \"work/visual-imagination/CLIP/class_sculpture television buddha.txt\",\n",
    "# \"work/visual-imagination/CLIP/class_a buddha sculpture with television in the grass.txt\",\n",
    "# \"work/visual-imagination/CLIP/class_television buddha sculpture with grass.txt\",\n",
    "# \"work/visual-imagination/CLIP/class_old television in grass with buddha sculpture by pacific ocean.txt\",\n",
    "# \"work/visual-imagination/CLIP/class_old television in long grass with buddha sculpture by pacific ocean.txt\"\n",
    "# ]\n",
    "\n",
    "# noise_filenames = [\n",
    "# \"work/visual-imagination/CLIP/noise_sculpture television buddha.txt\",\n",
    "# \"work/visual-imagination/CLIP/noise_a buddha sculpture with television in the grass.txt\",\n",
    "# \"work/visual-imagination/CLIP/noise_television buddha sculpture with grass.txt\",\n",
    "# \"work/visual-imagination/CLIP/noise_old television in grass with buddha sculpture by pacific ocean.txt\",\n",
    "# \"work/visual-imagination/CLIP/noise_old television in long grass with buddha sculpture by pacific ocean.txt\"\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     'a photo of wild tarragon',\n",
    "#     'a drawing of wild tarragon, a tasteless planhttps://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.htmlt',\n",
    "#     'a painting of farm hands, a kind of laborer',\n",
    "#     'a painting of a farmer’s hands',\n",
    "#     'a self-portrait of Artemisia Gentileschi, artist',\n",
    "#     'artemisia Gentileschi is a dragon',\n",
    "#     'a painting of Artemisia Gentileschi as a dragon',\n",
    "#     'a photo of the dragon Artemisia Gentileschi',\n",
    "#     'a portrait of artist as dragon',\n",
    "#     'a drawing of a dragon',\n",
    "#     'a painting of uprooted rhizome as a dragon',\n",
    "#     'a sketch of a rhizome, uprooted',\n",
    "#     'an image of a plant rising',\n",
    "#     'a drawing of plant roots and mycorrhizal fungi',\n",
    "#     'an image of growing wiser',\n",
    "#     'a painting of wise plants',\n",
    "#     'a drawing of plant wisdom',\n",
    "#     'a photo of a plant hiding',\n",
    "#     'a drawing of hiding from elders',\n",
    "#     'a painting of Susanna and the Elders',\n",
    "#     'an image of creeps',\n",
    "#     'a painting of gazing creeps',\n",
    "#     'a painting of groping creeps',\n",
    "#     'a painting of invasive elders',\n",
    "#     'a photo of perverse hope',\n",
    "#     'a painting of your hatred',\n",
    "#     'a drawing of killing a mosquito',\n",
    "#     'a painting of a mosquito, a kind of corpse',\n",
    "#     'a drawing of malaria',\n",
    "#     'a sketch of salted fields',\n",
    "#     'a photo of dancers',\n",
    "#     'a painting of dancers in a field',\n",
    "#     'an image of your spit',\n",
    "#     'a photo of standing too close',\n",
    "#     'a painting of someone standing too close',\n",
    "#     'a drawing of an oak sapling',\n",
    "#     'a painting of an oak in an empty field',\n",
    "#     'a photo of growing',\n",
    "#     'an image of growing wilder',\n",
    "#     'a painting of growing stronger',\n",
    "#     'a photo of a hand holding high',\n",
    "#     'a painting of a hand holding the head of Holofernes',\n",
    "#     'a painting of the head of Holofernes',\n",
    "#     'a drawing of a head, blood-rooted',\n",
    "#     'an image of a bloody root',\n",
    "#     'a painting of autumn gold',\n",
    "#     'a photo of a golden gown',\n",
    "#     'an image of a mouth tasting',\n",
    "#     'a sketch of a mouth',\n",
    "#     'a drawing of taste',\n",
    "#     'a painting of the taste of nothing',\n",
    "#     'a photograph of being invisible',\n",
    "#     'a drawing of your renown',\n",
    "#     'a painting of a renowned artist',\n",
    "#     'a portrait of the artist',\n",
    "#     'a self-portrait of Artemisia Gentileschi as tarragon'\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     'a picture of the interior of a bedroom',\n",
    "#     'a drawing of the interior of a bedroom', \n",
    "#     'a photo of the interior of a bedroom', \n",
    "#     'a photo of a bathroom',\n",
    "#     'a photo of a kitchen', \n",
    "#     'a photo of a front door'\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     'a photo of a front door',\n",
    "#     'a picture of the mud room',\n",
    "#     'a photo of the kitchen', \n",
    "#     'a photo of the livingroom with television', \n",
    "#     'a photo of a couch and a television', \n",
    "#     'a photo of a family on a couch', \n",
    "#     'a picture a kitchen', \n",
    "#     'a photo of food on the kitchen counter',\n",
    "#     'a photo of a pie in the oven',\n",
    "#     'a photo of a bathroom',\n",
    "#     'a photo of the interior of a bathroom',\n",
    "#     'a photo of a person in a shower', \n",
    "#     'a photo of person brushing their teeth',\n",
    "#     'a picture of the interior of a bedroom',\n",
    "#     'a picture of a person sleeping in a bed', \n",
    "#     'a photo of a sunrise through a window'\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     \"sunrise through a window\",\n",
    "#     \"a cat in the refrigerator\"\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     \"over my head, I see the bronze butterfly\",\n",
    "#     \"asleep on the black trunk\",\n",
    "#     \"blowing like a leaf in green shadow\",   \n",
    "#     \"down the ravine behind the empty house\",   \n",
    "#     \"the cowbells follow one another\",   \n",
    "#     \"into the distances of the afternoon\",   \n",
    "#     \"to my right\",\n",
    "#     \"in a field of sunlight between two pines\",   \n",
    "#     \"the droppings of last year’s horses\",   \n",
    "#     \"blaze up into golden stones\",\n",
    "#     \"I lean back, as the evening darkens and comes on\",\n",
    "#     \"a chicken hawk floats over, looking for home\",\n",
    "#     \"I have wasted my life\"\n",
    "# ]\n",
    "\n",
    "# prompts = [\n",
    "#     \"Midway on our lifes journey, I found myself\",\n",
    "#     \"In dark woods, the right road lost\",\n",
    "#     \"To tell About those woods is hard - so tangled and rough\",\n",
    "#     \"And savage that thinking of it now, I feel\",\n",
    "#     \"The old fear stirring: death is hardly more bitter.\",\n",
    "#     \"And yet, to treat the good I found there as well\",\n",
    "#     \"I'll tell what I saw, though how I came to enter\",\n",
    "#     \"I cannot well say, being so full of sleep\",\n",
    "#     \"Whatever moment it was I began to blunder\",\n",
    "#     \"Off the true path. But when I came to stop\",\n",
    "#     \"Below a hill that marked one end of the valley\",\n",
    "#     \"That had pierced my heart with terror, I looked up\",\n",
    "#     \"Toward the crest and saw its shoulders already\",\n",
    "#     \"Mantled in rays of that bright planet that shows\",\n",
    "#     \"The road to everyone, whatever our journey.\",\n",
    "#     \"Then I could feel the terror begin to ease\",\n",
    "#     \"That churned in my heart's lake all through the night.\",\n",
    "#     \"As one still panting, ashore from dangerous seas\",\n",
    "#     \"Looks back at the deep he has escaped, my thought\",\n",
    "#     \"Returned, still fleeing, to regard that grim defile\", \n",
    "#     \"That never left any alive who stayed in it.\"\n",
    "# ]\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "WanderCLIP.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
